{"cells":[{"cell_type":"markdown","source":["## Seven Reasons To Learn PyTorch on Databricks\nWhat expedites the process of learning new concepts, languages, or systems? Or, when learning a new task, do you look for analogues from skills you already possess?\n\nAcross all learning endeavors, three favorable characteristics stand out: familiarity, clarity, and simplicity. Familiarity eases the transition because of a recognizable link between the old and new ways of doing. Clarity minimizes the cognitive burden. And Simplicity reduces the friction in the adoption of the unknown and, as a result, it increases the fruition of learning a new concept, language, or system.\n\nKeeping these three characteristics in mind, we examine in this blog several reasons why it's easy to learn PyTorch and how the [Databricks Lakehouse Platform](https://databricks.com/product/data-lakehouse) facilitates the learning process.\n\n<table>\n  <tr><td>\n    <img src=\"https://raw.githubusercontent.com/dmatrix/data-assets/main/images/7_reasons_to_learn_pytorch.png\"\n         alt=\"7 Reasons to Learn PyTorch on Databricks\" width=\"800\" align=\"middle\">\n  </td></tr>\n</table>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0d7f00e-815c-44e9-a59b-e5e228ea67e5"}}},{"cell_type":"markdown","source":["### 1a. PyTorch is _Pythonic_\n\nLuciano Ramalho in Fluent Python defines Pythonic as an idiomatic way to use Python code that makes use of language features to be concise and readable. Python object constructs follow a certain protocol, and their behaviors adhere to a consistent pattern across classes, iterators, generators, sequences, context managers, modules, coroutines, decorators, etc. Even with little familiarity with the [Python data model](https://docs.python.org/3/reference/datamodel.html), modules, and language constructs, you recognize similar constructs in [PyTorch APIs](https://pytorch.org/docs/stable/nn.html), such as a `torch.tensor`, `torch.nn.Module`, `torch.utils.data.Datasets`, `torch.utils.data.DataLoaders`, etc. Not only do you see this Pythonic familiarity in PyTorch but also in other PyData ecosystem packages.\n\nPyTorch integrates with the PyData ecosystem, so your familiarity with [NumPy](https://numpy.org/) makes the transition easy to learn [Torch Tensors](https://pytorch.org/docs/stable/tensors.html). Numpy arrays and Tensors have similar data structures and operations. Just as [DataFrames](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html?highlight=dataframes) are central data structures to [Apache Spark™](https://spark.apache.org/) operations, so are tensors as inputs to PyTorch models, training operations, computations, and scoring. A PyTorch tensor’s mental image (shown in the diagram below) maps to an n-dimensional numpy array.\n\n<table>\n  <tr><td>\n    <img src=\"https://raw.githubusercontent.com/dmatrix/data-assets/main/images/tensors.png\"\n         alt=\"Tensors in PyTorch\\\" width=\"400\">\n  </td></tr>\n</table>\n\n\nFor instance, you can seamlessly create Numpy arrays and convert them into Torch tensors. Such familiarity of Numpy operations transfers easily to tensor operations, too, as you can observe from our simple operations on both Numpy and Tensors in the code below. \n\nBoth have familiar, imperative, and intuitive operations that one would expect from Python object APIs, such as lists, tuples, dictionaries, sets, etc. All this familiarity with Numpy's equivalent array operations on Torch tensors helps. Consider these examples:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c259817a-a67c-402e-a19d-5cedbf041752"}}},{"cell_type":"code","source":["import torch\nimport numpy as np\n\n# Create a numpy array of 2-dimension\nx_np = np.array([[1, 2, 3], [4, 5, 6]], np.int32)\ny_np = np.array([[2, 4, 6], [8, 10, 12]], np.int32)\nprint(\"x_shape: {}, y_shape: {}, x and y dimensions: {}, {}\". format(x_np.shape,y_np.shape, x_np.ndim, y_np.ndim))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91a396e9-8a20-4681-b7ba-36452ac733f2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">x_shape: (2, 3), y_shape: (2, 3), x and y dimensions: 2, 2\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">x_shape: (2, 3), y_shape: (2, 3), x and y dimensions: 2, 2\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Convert numpy array to a 2-rank tensor\nx_t = torch.from_numpy(x_np)\ny_t = torch.from_numpy(y_np)\nprint(\"x tensor: {}, y tensor {}, x and y tensor ranks: {}, {}\".format(x_t, y_t, x_t.ndim, y_t.ndim))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20b28593-12cb-4d31-88ee-0b3324fd9d52"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">x tensor: tensor([[1, 2, 3],\n        [4, 5, 6]], dtype=torch.int32), y tensor tensor([[ 2,  4,  6],\n        [ 8, 10, 12]], dtype=torch.int32), x and y tensor ranks: 2, 2\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">x tensor: tensor([[1, 2, 3],\n        [4, 5, 6]], dtype=torch.int32), y tensor tensor([[ 2,  4,  6],\n        [ 8, 10, 12]], dtype=torch.int32), x and y tensor ranks: 2, 2\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Add two numpy array and two tensors. The methods names are similar\nxy_np = np.add(x_np, y_np) \nxy_t = torch.add(x_t, y_t)\nprint(\"Addition: Numpy array xy_np: {}, Tensors xy_t: {}\".format(xy_np, xy_t))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f72255c3-d6f8-4a6c-8113-4e0ea8d29700"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Addition: Numpy array xy_np: [[ 3  6  9]\n [12 15 18]], Tensors xy_t: tensor([[ 3,  6,  9],\n        [12, 15, 18]], dtype=torch.int32)\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Addition: Numpy array xy_np: [[ 3  6  9]\n [12 15 18]], Tensors xy_t: tensor([[ 3,  6,  9],\n        [12, 15, 18]], dtype=torch.int32)\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 1b. Easy to Extend PyTorch  _nn_ Modules\n\nPyTorch library includes [neural network modules](https://pytorch.org/docs/stable/nn.html) to build a layered network architecture. In PyTorch parlance, these modules comprise each layer of your network. Derived from its base class module `torch.nn.Module`, you can easily create a simple or complex layered neural network. To define a PyTorch customized network module class and its methods, you follow a similar pattern to build a customized Python object class derived from its base class object. Let's define a simple [two-layered](https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html#pytorch-custom-nn-modules) linear network example, to illustrate this similarity.\n\nNotice that the custom `TwoLayeredNet` below is Pythonic in its flow and structure. Derived classes from the torch.nn.Module have class initializers with parameters, define interface methods, and they are callable. That is, the base class `nn.Module` implements the Python magic `__call__()` object method. Even though the two-layered model is simple, it demonstrates this familiarity with extending a class from Python’s base object. \n\nFurthermore, you get an intuitive feeling that you are writing or reading Python application code while using PyTorch APIs. It does not feel like you're learning a new language: the syntax, structure, form, and behavior are all too familiar; the unfamiliar bit are the PyTorch modules and the APIs, which are no different when learning a new PyData package APIs and incorporating their use in your Python application code."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9577d145-0bdd-43a5-b640-e02f430e6c3c"}}},{"cell_type":"code","source":["import torch\nimport torch.nn as nn\n\nclass TwoLayerNet(nn.Module):\n    \"\"\"\n    In the constructor we instantiate two nn.Linear modules and assign them as\n    member variables.\n    \"\"\"\n    def __init__(self, input_size, hidden_layers, output_size):\n        super(TwoLayerNet, self).__init__()\n        self.l1 = nn.Linear(input_size, hidden_layers)\n        self.relu = nn.ReLU()\n        self.l2 = nn.Linear(hidden_layers, output_size)\n        \n    def forward(self, x):\n        \"\"\"\n        In the forward function we accept a Tensor of input data and we must return\n        a Tensor of output data. We can use Modules defined in the constructor as\n        well as arbitrary (differentiable) operations on Tensors.\n        \"\"\"\n        y_pred = self.l1(x)\n        y_pred = self.relu(y_pred)\n        y_pred = self.l2(y_pred)\n        \n        return y_pred"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f930652-fab6-43e6-878c-861d1eae642a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Define some input and out dimensions to the network layer and check if `cuda` is available"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b74983ea-8c22-4ccc-976a-887da8c00f11"}}},{"cell_type":"code","source":["dtype = torch.float\n# Check if we can use cuda if GPUs are avaliable\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# N is batch size; D_in is input dimension;\n# H is hidden dimension layer; D_out is output dimension.\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# Create random Tensors to use inputs and outputs on respective CPU or GPU processors\nX = torch.randn(N, D_in, device=device, dtype=dtype)\ny = torch.randn(N, D_out, device=device, dtype=dtype)\nprint(\"X shape: {}, Y shape: {}, X rank: {}, Y rank: {}\". format(X.shape, y.shape, X.ndim, y.ndim))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66cd2e42-c43a-49c4-9cc7-d394ce904c6c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">X shape: torch.Size([64, 1000]), Y shape: torch.Size([64, 10]), X rank: 2, Y rank: 2\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">X shape: torch.Size([64, 1000]), Y shape: torch.Size([64, 10]), X rank: 2, Y rank: 2\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Construct our model by instantiating the class defined above, as you would construct any Python custom class object."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffbdba05-c14e-49da-ba92-58c8a15bdf60"}}},{"cell_type":"code","source":["# Check if CUDA is available for GPUs. \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = TwoLayerNet(D_in, H, D_out).to(device)\nmodel, device"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"64d13592-815d-4fc8-9e30-4b3b954d1764"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[6]: (TwoLayerNet(\n   (l1): Linear(in_features=1000, out_features=100, bias=True)\n   (relu): ReLU()\n   (l2): Linear(in_features=100, out_features=10, bias=True)\n ),\n device(type=&#39;cpu&#39;))</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: (TwoLayerNet(\n   (l1): Linear(in_features=1000, out_features=100, bias=True)\n   (relu): ReLU()\n   (l2): Linear(in_features=100, out_features=10, bias=True)\n ),\n device(type=&#39;cpu&#39;))</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Construct our loss function and an Optimizer. The call to `model.parameters()` in the SGD constructor will contain the learnable parameters of the two\n`nn.Linear modules` which are members of the model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad179124-8be8-4e08-8189-8ed028a95adb"}}},{"cell_type":"code","source":["learning_rate = 1e-4\nloss_fn = torch.nn.MSELoss(reduction='sum')\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94ff52be-d3f8-469b-a552-b019d378bcbf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now we define a simple training loop with some iterations, using Python familiar language constructs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b35bafb-8ac1-4ec2-a127-2a1704abf8a1"}}},{"cell_type":"code","source":["for t in range(350):\n    \n    # Forward pass: Compute predicted y by passing x to the model.\n    # invokde model object since it's callable to compute the predictions\n    y_pred = model(X)\n    \n    # Compute and print loss\n    loss = loss_fn(y_pred, y)\n    if t % 50 == 0:\n      print(\"iterations: {}, loss: {:8.2f}\".format(t, loss.item()))\n    \n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2137dc56-d563-4f23-9b53-3f88dc8287a3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">iterations: 0, loss:   671.34\niterations: 50, loss:    29.44\niterations: 100, loss:     1.83\niterations: 150, loss:     0.18\niterations: 200, loss:     0.02\niterations: 250, loss:     0.00\niterations: 300, loss:     0.00\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">iterations: 0, loss:   671.34\niterations: 50, loss:    29.44\niterations: 100, loss:     1.83\niterations: 150, loss:     0.18\niterations: 200, loss:     0.02\niterations: 250, loss:     0.00\niterations: 300, loss:     0.00\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["What follows from above is a recognizable pattern and flow between how you define Python’s customized class and a simple PyTorch neural network. Also, the code is concise and reads like Python code. Another recognizable Pythonic pattern in PyTorch is how `Dataset` and `DataLoaders` use Python protocols to build iterators."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"252831e6-08a6-44fc-a9b7-c5f1aa445c7c"}}},{"cell_type":"markdown","source":["## 1c. Easy to Customize PyTorch Dataset for Dataloaders\nAt the core of PyTorch data loading utility is the `torch.utils.data.DataLoader` class; they are an integral part of the PyTorch iterative training process, in which we iterate over batches of input during an epoch of training. `DataLoaders` offer a Python iterable over your custom dataset by implementing a Python sequence and iterable protocol: this includes implementing `__len__` and `__getitem__` magic methods on an object. Again, very Pythonic in behavior: as part of the implementation, we employ list comprehensions, use numpy arrays to convert to tensors, and use random access to fetch _nth_ data item—all conforming to familiar access patterns and behaviors of doing things in Python.\n\nLet's look at a simple custom Dataset of temperatures for use in training a model. Other complex datasets could be images, extensive features datasets of tensors, etc."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f4bcff2-c841-4715-98f0-10a4c5e13ecf"}}},{"cell_type":"code","source":["import math\nfrom torch.utils.data import Dataset, DataLoader\n\nclass FahrenheitTemperatures(Dataset):\n    def __init__(self, start=0, stop=212, size=5000):\n        super(FahrenheitTemperatures, self).__init__()\n        \n        # Intialize local variables and covert them into tensors\n        f_temp = np.random.randint(start, high=stop, size=size)\n        # Use Python list comprehension to convert centrigrade\n        c_temp = np.array([self._f2c(f) for f in f_temp])\n        # Convert to Tensors from numpy\n        self.X = torch.from_numpy(f_temp).float()\n        self.y = torch.from_numpy(c_temp).float()\n        # Data for prediction or validation\n        self.X_pred = torch.from_numpy(np.arange(212, 170, -5, dtype=float))\n        self.n_samples = self.X.shape[0]\n        \n    def __getitem__(self, index):\n        # Support indexing such that dataset[i] can be used to get i-th sample\n        # implement this python function for indexing\n        # return a tuple (X,y)\n        return self.X[index], self.y[index]\n        \n        \n    def __len__(self):\n        # We can call len(dataset) to return the size, so this can be used\n        # as an iterator\n        return self.n_samples\n    \n    def _f2c(sel,f) -> float:\n        return (f - 32) * 5.0/9.0"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"532ee9a1-a601-47f3-8baa-57838aea16cd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Using familiar Python access patterns, you can use [] to access your data for a given integer index, since we have implemented the `__getitem__` magic method."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"064553c6-eb82-4bae-8ad6-342941372508"}}},{"cell_type":"code","source":["# Let's now access our dataset using an index\ndataset = FahrenheitTemperatures()\n#unpack since it returns a tuple\nfeatures, labels = dataset[0]\nprint('Fahrenheit: {:.2f}'.format(features))\nprint('Celcius   : {:.2f}'.format(labels))\nprint('Samples: {}'.format(len(dataset)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"381bfc5b-80fb-4aa1-8aa2-3a9a33256fa3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Fahrenheit: 65.00\nCelcius   : 18.33\nSamples: 5000\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Fahrenheit: 65.00\nCelcius   : 18.33\nSamples: 5000\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["A PyTorch DataLoader class takes an instance of a customized `FahrenheitTemperatures` class object as a parameter. This utility class is standard in PyTorch training loops. It offers an ability to iterate over batches of data like an iterator: again, a very _Pythonic_ and straightforward way of doing things!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af589589-64a8-42dc-9a7e-275dbc7d7aea"}}},{"cell_type":"code","source":["# Let's try Dataloader class and make this into an iterator and access the data as above\ndataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True)\n\ndataiter = iter(dataloader)\ndata = dataiter.next()\n# Since we specified our batch size to be 4, we'll see four features and labels\nprint('Fahrenheit: {}'.format(data[0]))\nprint('Celcius   : {}'.format(data[1]))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b20f6d2d-3157-4e6b-abe5-06eeb23bb0cf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Fahrenheit: tensor([210.,  54., 101.,   0.])\nCelcius   : tensor([ 98.8889,  12.2222,  38.3333, -17.7778])\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Fahrenheit: tensor([210.,  54., 101.,   0.])\nCelcius   : tensor([ 98.8889,  12.2222,  38.3333, -17.7778])\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Since we implemented our custom `Dataset`, let's use it in the PyTorch training loop."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6dcae1e-e63e-400f-886d-f8db528df157"}}},{"cell_type":"code","source":["# Let's do a dummy training loop\nnum_epochs = 2\nbatch_size = 4\ntotal_samples = len(dataset)\nn_iterations = math.ceil(total_samples/batch_size)\nfor epoch in range(num_epochs):\n    # iterate over our dataloader in batches\n    # Because we have implemented our Dataset class with __getitem__ and __len__, we\n    # can iterate over it\n    for i, (inputs, labels) in enumerate(dataloader):\n        # Torward and backward pass, update gradients, and zero them out\n        # would appear within this loop\n        # Run your training process\n        if (i+1) % 400 == 0:\n            print(f'Epoch: {epoch+1}/{num_epochs}, Step {i+1}/{n_iterations}| Inputs {inputs.shape} | Labels {labels.shape}, Tensors {inputs}')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84e327db-fbf6-430e-9448-f3ac764a03ed"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Epoch: 1/2, Step 400/1250| Inputs torch.Size([4]) | Labels torch.Size([4]), Tensors tensor([120.,  25., 166.,  36.])\nEpoch: 1/2, Step 800/1250| Inputs torch.Size([4]) | Labels torch.Size([4]), Tensors tensor([102., 155., 104., 145.])\nEpoch: 1/2, Step 1200/1250| Inputs torch.Size([4]) | Labels torch.Size([4]), Tensors tensor([163., 197., 179., 143.])\nEpoch: 2/2, Step 400/1250| Inputs torch.Size([4]) | Labels torch.Size([4]), Tensors tensor([ 48., 104.,  47.,   4.])\nEpoch: 2/2, Step 800/1250| Inputs torch.Size([4]) | Labels torch.Size([4]), Tensors tensor([ 59., 179., 163., 152.])\nEpoch: 2/2, Step 1200/1250| Inputs torch.Size([4]) | Labels torch.Size([4]), Tensors tensor([ 21., 192.,  90.,  55.])\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Epoch: 1/2, Step 400/1250| Inputs torch.Size([4]) | Labels torch.Size([4]), Tensors tensor([120.,  25., 166.,  36.])\nEpoch: 1/2, Step 800/1250| Inputs torch.Size([4]) | Labels torch.Size([4]), Tensors tensor([102., 155., 104., 145.])\nEpoch: 1/2, Step 1200/1250| Inputs torch.Size([4]) | Labels torch.Size([4]), Tensors tensor([163., 197., 179., 143.])\nEpoch: 2/2, Step 400/1250| Inputs torch.Size([4]) | Labels torch.Size([4]), Tensors tensor([ 48., 104.,  47.,   4.])\nEpoch: 2/2, Step 800/1250| Inputs torch.Size([4]) | Labels torch.Size([4]), Tensors tensor([ 59., 179., 163., 152.])\nEpoch: 2/2, Step 1200/1250| Inputs torch.Size([4]) | Labels torch.Size([4]), Tensors tensor([ 21., 192.,  90.,  55.])\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Although the aforementioned Pythonic reasons are not directly related to [Databricks Lakehouse Platform](https://databricks.com/product/data-lakehouse), they account for ideas of familiarity, clarity, simplicity, and the _Pythonic_ way of writing PyTorch code. Next, we examine what aspects within the Databricks Lakehouse Platform’s runtime for machine learning facilitate learning PyTorch."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"489eeb40-fe31-46d5-a205-112335f8c833"}}},{"cell_type":"markdown","source":["## 2. No need to install Python packages\nAs part of the Databricks Lakehouse platform, the runtime for machine learning (MLR) comes preinstalled with the latest versions of Python, PyTorch, PyData ecosystem packages, and additional standard machine learning libraries saving you from installing or managing any packages. Out-of-the-box and ready-to-use-runtime environments are conducive to learning because they reduce the friction to get started by unburdening you to control or install packages. If you want to install additional Python packages, it's as simple as using ``%pip install <package_name>``. This ability to support [package management](How to Simplify Python Environment Management Using Databricks’ %pip and %conda Magic Commands) on your cluster is popular among Databricks customers and widely used as part of their development model lifecycle.\n\nTo inspect the list of all preinstalled packages, use the `%pip list`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d9e452d-ef24-4444-822d-f94398aef538"}}},{"cell_type":"code","source":["%pip list"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd33980f-8070-4d04-9d0e-bd05a4d7e169"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Package                      Version\n---------------------------- -------------------\nabsl-py                      0.11.0\naiohttp                      3.6.3\nasn1crypto                   1.4.0\nastor                        0.8.1\nastunparse                   1.6.3\nasync-timeout                3.0.1\nattrs                        20.3.0\nazure-core                   1.10.0\nazure-storage-blob           12.7.1\nbackcall                     0.2.0\nbcrypt                       3.2.0\nblinker                      1.4\nboto3                        1.16.7\nbotocore                     1.19.7\nbrotlipy                     0.7.0\ncachetools                   4.2.1\ncertifi                      2020.12.5\ncffi                         1.14.3\nchardet                      3.0.4\nclick                        7.1.2\ncloudpickle                  1.6.0\nconfigparser                 5.0.1\ncryptography                 3.1.1\ncycler                       0.10.0\nCython                       0.29.21\ndatabricks-cli               0.14.1\ndecorator                    4.4.2\ndill                         0.3.2\ndiskcache                    5.2.1\ndocker                       4.4.1\ndocutils                     0.15.2\nentrypoints                  0.3\nFlask                        1.1.2\nflatbuffers                  1.12\nfuture                       0.18.2\ngast                         0.3.3\ngitdb                        4.0.5\nGitPython                    3.1.11\ngoogle-auth                  1.22.1\ngoogle-auth-oauthlib         0.4.2\ngoogle-pasta                 0.2.0\ngrpcio                       1.32.0\ngunicorn                     20.0.4\nh5py                         2.10.0\nhorovod                      0.21.1\nidna                         2.10\nimportlib-metadata           2.0.0\nipykernel                    5.3.4\nipython                      7.19.0\nipython-genutils             0.2.0\nisodate                      0.6.0\nitsdangerous                 1.1.0\njedi                         0.17.2\nJinja2                       2.11.2\njmespath                     0.10.0\njoblib                       0.17.0\njoblibspark                  0.3.0\njupyter-client               6.1.7\njupyter-core                 4.6.3\nKeras-Preprocessing          1.1.2\nkiwisolver                   1.3.0\nkoalas                       1.5.0\nlightgbm                     3.1.1\nllvmlite                     0.35.0\nMako                         1.1.3\nMarkdown                     3.3.2\nMarkupSafe                   1.1.1\nmatplotlib                   3.2.2\nmkl-fft                      1.2.0\nmkl-random                   1.1.0\nmkl-service                  2.3.0\nmleap                        0.16.1\nmlflow                       1.13.1\nmore-itertools               8.6.0\nmsrest                       0.6.21\nmultidict                    4.7.6\nnetworkx                     2.5\nnltk                         3.5\nnumba                        0.52.0\nnumpy                        1.19.2\noauthlib                     3.1.0\nolefile                      0.46\nopt-einsum                   3.3.0\npackaging                    20.4\npandas                       1.1.3\nparamiko                     2.7.2\nparso                        0.7.0\npatsy                        0.5.1\npetastorm                    0.9.8\npexpect                      4.8.0\npickleshare                  0.7.5\nPillow                       8.0.1\npip                          20.2.4\nplotly                       4.14.2\nprompt-toolkit               3.0.8\nprotobuf                     3.13.0\npsutil                       5.7.2\npsycopg2                     2.8.5\nptyprocess                   0.6.0\npyarrow                      1.0.1\npyasn1                       0.4.8\npyasn1-modules               0.2.8\npycparser                    2.20\nPygments                     2.7.2\nPyJWT                        1.7.1\nPyNaCl                       1.4.0\npyodbc                       4.0.0-unsupported\npyOpenSSL                    19.1.0\npyparsing                    2.4.7\nPySocks                      1.7.1\npython-dateutil              2.8.1\npython-editor                1.0.4\npytz                         2020.1\nPyYAML                       5.4.1\npyzmq                        19.0.2\nquerystring-parser           1.2.4\nregex                        2020.10.15\nrequests                     2.24.0\nrequests-oauthlib            1.3.0\nretrying                     1.3.3\nrsa                          4.7\ns3transfer                   0.3.4\nscikit-learn                 0.23.2\nscipy                        1.5.2\nseaborn                      0.10.0\nsetuptools                   50.3.1.post20201107\nshap                         0.37.0\nsimplejson                   3.17.2\nsix                          1.15.0\nslicer                       0.0.3\nsmmap                        3.0.5\nspark-tensorflow-distributor 0.1.0\nsqlparse                     0.4.1\nstatsmodels                  0.12.0\ntabulate                     0.8.7\ntensorboard                  2.4.1\ntensorboard-plugin-wit       1.8.0\ntensorflow-cpu               2.4.0\ntensorflow-estimator         2.4.0\ntermcolor                    1.1.0\nthreadpoolctl                2.1.0\ntorch                        1.7.1\ntorchvision                  0.8.2\ntornado                      6.0.4\ntqdm                         4.50.2\ntraitlets                    5.0.5\ntyping-extensions            3.7.4.3\nurllib3                      1.25.11\nwcwidth                      0.2.5\nwebsocket-client             0.57.0\nWerkzeug                     1.0.1\nwheel                        0.35.1\nwrapt                        1.12.1\nxgboost                      1.3.1\nyarl                         1.6.3\nzipp                         3.4.0\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Package                      Version\n---------------------------- -------------------\nabsl-py                      0.11.0\naiohttp                      3.6.3\nasn1crypto                   1.4.0\nastor                        0.8.1\nastunparse                   1.6.3\nasync-timeout                3.0.1\nattrs                        20.3.0\nazure-core                   1.10.0\nazure-storage-blob           12.7.1\nbackcall                     0.2.0\nbcrypt                       3.2.0\nblinker                      1.4\nboto3                        1.16.7\nbotocore                     1.19.7\nbrotlipy                     0.7.0\ncachetools                   4.2.1\ncertifi                      2020.12.5\ncffi                         1.14.3\nchardet                      3.0.4\nclick                        7.1.2\ncloudpickle                  1.6.0\nconfigparser                 5.0.1\ncryptography                 3.1.1\ncycler                       0.10.0\nCython                       0.29.21\ndatabricks-cli               0.14.1\ndecorator                    4.4.2\ndill                         0.3.2\ndiskcache                    5.2.1\ndocker                       4.4.1\ndocutils                     0.15.2\nentrypoints                  0.3\nFlask                        1.1.2\nflatbuffers                  1.12\nfuture                       0.18.2\ngast                         0.3.3\ngitdb                        4.0.5\nGitPython                    3.1.11\ngoogle-auth                  1.22.1\ngoogle-auth-oauthlib         0.4.2\ngoogle-pasta                 0.2.0\ngrpcio                       1.32.0\ngunicorn                     20.0.4\nh5py                         2.10.0\nhorovod                      0.21.1\nidna                         2.10\nimportlib-metadata           2.0.0\nipykernel                    5.3.4\nipython                      7.19.0\nipython-genutils             0.2.0\nisodate                      0.6.0\nitsdangerous                 1.1.0\njedi                         0.17.2\nJinja2                       2.11.2\njmespath                     0.10.0\njoblib                       0.17.0\njoblibspark                  0.3.0\njupyter-client               6.1.7\njupyter-core                 4.6.3\nKeras-Preprocessing          1.1.2\nkiwisolver                   1.3.0\nkoalas                       1.5.0\nlightgbm                     3.1.1\nllvmlite                     0.35.0\nMako                         1.1.3\nMarkdown                     3.3.2\nMarkupSafe                   1.1.1\nmatplotlib                   3.2.2\nmkl-fft                      1.2.0\nmkl-random                   1.1.0\nmkl-service                  2.3.0\nmleap                        0.16.1\nmlflow                       1.13.1\nmore-itertools               8.6.0\nmsrest                       0.6.21\nmultidict                    4.7.6\nnetworkx                     2.5\nnltk                         3.5\nnumba                        0.52.0\nnumpy                        1.19.2\noauthlib                     3.1.0\nolefile                      0.46\nopt-einsum                   3.3.0\npackaging                    20.4\npandas                       1.1.3\nparamiko                     2.7.2\nparso                        0.7.0\npatsy                        0.5.1\npetastorm                    0.9.8\npexpect                      4.8.0\npickleshare                  0.7.5\nPillow                       8.0.1\npip                          20.2.4\nplotly                       4.14.2\nprompt-toolkit               3.0.8\nprotobuf                     3.13.0\npsutil                       5.7.2\npsycopg2                     2.8.5\nptyprocess                   0.6.0\npyarrow                      1.0.1\npyasn1                       0.4.8\npyasn1-modules               0.2.8\npycparser                    2.20\nPygments                     2.7.2\nPyJWT                        1.7.1\nPyNaCl                       1.4.0\npyodbc                       4.0.0-unsupported\npyOpenSSL                    19.1.0\npyparsing                    2.4.7\nPySocks                      1.7.1\npython-dateutil              2.8.1\npython-editor                1.0.4\npytz                         2020.1\nPyYAML                       5.4.1\npyzmq                        19.0.2\nquerystring-parser           1.2.4\nregex                        2020.10.15\nrequests                     2.24.0\nrequests-oauthlib            1.3.0\nretrying                     1.3.3\nrsa                          4.7\ns3transfer                   0.3.4\nscikit-learn                 0.23.2\nscipy                        1.5.2\nseaborn                      0.10.0\nsetuptools                   50.3.1.post20201107\nshap                         0.37.0\nsimplejson                   3.17.2\nsix                          1.15.0\nslicer                       0.0.3\nsmmap                        3.0.5\nspark-tensorflow-distributor 0.1.0\nsqlparse                     0.4.1\nstatsmodels                  0.12.0\ntabulate                     0.8.7\ntensorboard                  2.4.1\ntensorboard-plugin-wit       1.8.0\ntensorflow-cpu               2.4.0\ntensorflow-estimator         2.4.0\ntermcolor                    1.1.0\nthreadpoolctl                2.1.0\ntorch                        1.7.1\ntorchvision                  0.8.2\ntornado                      6.0.4\ntqdm                         4.50.2\ntraitlets                    5.0.5\ntyping-extensions            3.7.4.3\nurllib3                      1.25.11\nwcwidth                      0.2.5\nwebsocket-client             0.57.0\nWerkzeug                     1.0.1\nwheel                        0.35.1\nwrapt                        1.12.1\nxgboost                      1.3.1\nyarl                         1.6.3\nzipp                         3.4.0\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##3. Easy to Use CPUs or GPUs\nNeural networks for deep learning involve numeric-intensive computations, including dot products and matrix multiplications on large and higher-ranked tensors. For these compute-bound PyTorch applications that require GPUs, you can easily create a cluster of MLR with GPUs and consign your data to use GPUs. As such, all your training can be done on GPUs, as the above simple example of `TwoLayeredNet` demonstrate how to use GPU for training if `cuda` is available.\n\nAlthough our example code below is simple, showing matrix multiplication of two randomly generated tensors, real PyTorch applications will have much more intense computation during their forward and backward passes and [auto-grad](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) computations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a5ebe2e-45f2-4bd0-9c76-7ba32b0001a9"}}},{"cell_type":"code","source":["dtype = torch.float\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# Randomly initialize weights and put the tensors on a GPU if available\na = torch.randn((5, 5), device=device, dtype=dtype)\nb = torch.randn((5, 5), device=device, dtype=dtype)\n# Matrix multiplication done on GPU\nc = torch.mul(a,b)\nprint(\" c: {}\".format(c))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8f450120-3ba5-4ff5-a20c-68e33ad60702"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"> c: tensor([[ 0.1512,  0.0265,  1.1087,  0.0544, -2.0399],\n        [ 0.0277,  0.2530,  1.4738, -0.1850,  3.9369],\n        [ 0.0074,  0.0412,  0.7437, -0.0600, -0.0488],\n        [-0.5553, -1.2324,  1.5299, -0.2288,  0.1586],\n        [ 0.5845,  0.1385,  0.9252,  0.1865,  0.0159]])\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"> c: tensor([[ 0.1512,  0.0265,  1.1087,  0.0544, -2.0399],\n        [ 0.0277,  0.2530,  1.4738, -0.1850,  3.9369],\n        [ 0.0074,  0.0412,  0.7437, -0.0600, -0.0488],\n        [-0.5553, -1.2324,  1.5299, -0.2288,  0.1586],\n        [ 0.5845,  0.1385,  0.9252,  0.1865,  0.0159]])\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 4. Easy to use TensorBoard"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2744e77c-4af9-4b29-be07-bd3163470d60"}}},{"cell_type":"markdown","source":["[Already announced in a blog](https://databricks.com/blog/2020/08/25/tensorboard-a-new-way-to-use-tensorboard-on-databricks.html) as part of the Databricks Runtime (DBR), this magic command displays your training metrics from [TensorBoard](https://www.tensorflow.org/tensorboard) within the same notebook. No longer do you need to leave your notebook and launch TensorBoard from another tab. This in-place visualization is a significant improvement toward simplicity and developer experience. And PyTorch developers can quickly see their metrics in TensorBoard.\n\nLet's try to run a sample [PyTorch FashionMNIST example](https://pytorch.org/docs/stable/tensorboard.html) with TensorBoard logging. \nFirst, define a `SummaryWriter`, followed by the FashionMNIST `Dataset` in the `DataLoader` in our PyTorch `torchvision.models.resnet50` model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f738883-5160-4bc1-bb40-c4cd67938e6b"}}},{"cell_type":"code","source":["from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\n\nwriter = SummaryWriter()\n\nfor n_iter in range(100):\n    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca6d55c7-c898-4b2c-9229-3cb00d31e7d2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import torch\nimport torchvision\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\n\n# Writer will output to ./runs/ directory by default\nwriter = SummaryWriter()\n\n# Transformation pipeline applied to the input data\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n# Create a PyTorch FashionMNIST dataset\ntrainset = datasets.FashionMNIST('mnist_train', train=True, download=True, transform=transform)\n# Use the dataset as in the Dataloader\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\nmodel = torchvision.models.resnet50(False)\n\n# Have ResNet model take in grayscale rather than RGB\nmodel.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\nimages, labels = next(iter(trainloader))\n\ngrid = torchvision.utils.make_grid(images)\nwriter.add_image('images', grid, 0)\nwriter.add_graph(model, images)\nwriter.close()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78f56a82-5a07-42bb-be04-2ca28e69d72b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Using our Datarbicks notebook’s `%magic commands`, we can launch the TensorBoard within our cell and examine the training metrics and model outputs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9fcc1a8e-2136-4c5f-be37-397e736d6f25"}}},{"cell_type":"code","source":["%load_ext tensorboard"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e17dead7-d5c5-4861-951c-123560d501ff"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%tensorboard --logdir=./runs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1af84bb2-cebe-4ac8-940d-4e2eb49bd72a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## 5. PyTorch Integrated with MLflow\n\nIn our steadfast effort to make Databricks simpler, we enhanced [MLflow fluent tracking APIs](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.autolog) to autolog MLflow entities—metrics, tags, parameters, and artifacts—for supported machine learning libraries, including PyTorch Lightning. Through the MLflow UI, an integral part of the workspace, you can access all MLflow experiments via the `Experiment` icon in the upper right corner. All experiment runs during training are automatically logged to the MLflow tracking server. No need for you to explicitly use the tracking APIs to log MLflow entities, albeit it does not prevent you from tracking and logging any additional entities such as images, dictionaries, or text artifacts, etc.\n\nHere is a minimal example of a PyTorch Lightning FashionMNIST instance with just a training loop step (no validation, no testing). It illustrates how you can use MLflow to autolog MLflow entities, peruse the MLflow UI to inspect its runs from within this notebook, register the model, and [serve or deploy](https://docs.databricks.com/applications/mlflow/model-serving.html) it."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de59d6d8-36dc-41ba-9be2-5559d81f03a6"}}},{"cell_type":"code","source":["%pip install pytorch_lightning"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"257a54d6-415b-4bac-931e-c15511884c37"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import os\nimport pytorch_lightning as pl\nimport torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import FashionMNIST\nfrom pytorch_lightning.metrics.functional import accuracy\n\nimport mlflow.pytorch\nfrom mlflow.tracking import MlflowClient\n\nclass MNISTModel(pl.LightningModule):\n\n    def __init__(self):\n        super(MNISTModel, self).__init__()\n        self.l1 = torch.nn.Linear(28 * 28, 10)\n\n    def forward(self, x):\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\n\n    def training_step(self, batch, batch_nb):\n        x, y = batch\n        loss = F.cross_entropy(self(x), y)\n        acc = accuracy(loss, y)\n        self.log(\"train_loss\", loss, on_epoch=True)\n        self.log(\"acc\", acc, on_epoch=True)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=0.02)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75a9c97f-1215-4316-850d-372813acaa60"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Create the PyTorch model as you would create a Python class, use the FashionMNIST `DataLoader`, a PyTorch Lightning `Trainer`, and autolog all MLflow entities during its `trainer.fit()` method."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ce6daee-99be-47b6-9a16-7da5df487b72"}}},{"cell_type":"code","source":["mnist_model = MNISTModel()\n# Init DataLoader from FashionMNIST Dataset\ntrain_ds = FashionMNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())\ntrain_loader = DataLoader(train_ds, batch_size=32) \n\n# Initialize a trainer\ntrainer = pl.Trainer(max_epochs=20, progress_bar_refresh_rate=20)\n\n# Auto log all MLflow entities\nmlflow.pytorch.autolog()\n\n# Train the model\nwith mlflow.start_run() as run:\n  trainer.fit(mnist_model, train_loader)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d26fe51-794e-417a-9768-b03cabb2d855"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##6. Convert MLflow PyTorch-logged models to TorchScript\n[TorchScript](https://pytorch.org/docs/stable/jit.html) is a way to create serializable and optimizable models from PyTorch code. We can convert a PyTorch MLflow-logged model into a TorchScript format, save, and load (or deploy to) a high-performance and independent process. Or [deploy and serve on Databricks cluster](https://docs.databricks.com/applications/mlflow/model-serving.html) as an endpoint.\n\nThe process entails the following steps:\n * Create an MLflow PyTorch model\n * Compile the model using JIT and convert it to the TorchScript model\n * Log or save the TorchScript model\n * Load or deploy the TorchScript model\n\nWe have not included all the code here for brevity, but you can examine the sample code—[IrisClassification](https://github.com/mlflow/mlflow/blob/master/examples/pytorch/torchscript/IrisClassification/iris_classification.py) and [MNIST](https://github.com/mlflow/mlflow/blob/master/examples/pytorch/torchscript/MNIST/mnist_torchscript.py)—in the [GitHub MLflow examples](https://github.com/mlflow/mlflow/tree/master/examples/pytorch/torchscript) directory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c6f3d1d-0d97-4dba-9c1d-71e641f9ff54"}}},{"cell_type":"markdown","source":["## 7. Ready-to-run PyTorch Tutorials for Distributed Training"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eea5f94e-38b0-4b13-a209-4d0e11d1ea65"}}},{"cell_type":"markdown","source":["Lastly, you can use the Databricks Lakehouse MLR cluster to distribute your PyTorch training. We provide a set of tutorials that demonstrate a) how to set up a single node training and b) how to migrate to the [Horovod](https://horovod.readthedocs.io/en/stable/pytorch.html) library to distribute your training. Working through these tutorials equips you with how to apply distributed training for your PyTorch models. Ready-to-run and easy-to-import-notebooks into your cluster, these notebooks are an excellent stepping-stone to learn distributed training. Just follow the recommended setups and sit back and watch the model train…\n\nEach notebook provides a step-by-step guide to set up an MLR cluster, how to adapt your code to use either CPUs or GPUs, and train your models in a distributed fashion with the Horovod library.\n\n * [Train a simple PyTorch Model](https://docs.databricks.com/applications/mlflow/tracking-ex-pytorch.html#train-a-pytorch-model)\n * [Use a PyTorch on a Single Node](https://docs.databricks.com/applications/machine-learning/train-model/pytorch.html#use-pytorch-on-a-single-node)\n * [Single node PyTorch to distributed deep learning](https://docs.databricks.com/applications/machine-learning/train-model/distributed-training/mnist-pytorch.html#single-node-pytorch-to-distributed-deep-learning)\n * [Simplify data conversion from Apache Spark to PyTorch](https://databricks.com/notebooks/simple-aws/petastorm-spark-converter-pytorch.html)\n \nMoreover, the PyTorch community has [Learning with PyTorch Examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html) starter tutorials. You can just as simply enter the code into a Databricks notebook and run it on your MLR cluster as in a Python IDE. As you work through them, you get a feel for the _Pythonic_ nature of PyTorch: imperative and intuitive."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cae0af23-ca76-4305-b50c-abb02a2cc1b2"}}},{"cell_type":"markdown","source":["## Conclusion \n\nWe discussed what expedites the process of learning new concepts, languages, or systems. Through examples, we showed how three favorable characteristics—familiarity, clarity, and simplicity—facilitate learning new ways of doing things when you are familiar with a concept, language, or system. These characteristics—the old way of doing things—transfer quickly to the unknown—a new way of doing things. We illustrated how PyTorch is _Pythonic_, making the transition into the unknown world of PyTorch easier because it feels as though you are writing familiar, imperative Python application code: in style, idiom, and form, except using PyTorch objects and APIs with standard Python semantics and syntax.\n\nAnd finally, we outlined additional reasons why learning PyTorch is much easier on the Databricks Lakehouse machine learning runtime environment because the platform offers capabilities that reduce friction and provides support to learn and accelerate your productivity, along with a set of PyTorch tutorials you can work through."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2656c753-385b-4a3d-9a00-9afea1119c62"}}},{"cell_type":"markdown","source":["## What’s Next: How to get started\n\nYou can try this notebook in your MLR cluster and import the PyTorch tutorials mentioned in this notebook. If you don't have a Databricks account, get one today for a free trial and have a go at PyTorch on Databricks Lakehouse Platform. For single-node training, limited functionality, and only CPUs usage, you can use the [Databricks Community Edition](https://databricks.com/try-databricks)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e5f4588-51b7-4753-841d-2af6ffd3520a"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"SevenReasonsToLearnPyTorchOnDatabricks","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":9472894}},"nbformat":4,"nbformat_minor":0}
